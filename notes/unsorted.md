The general idea is to simulate communication between agents, but to let them be as free as possible in choosing their mode of communication.

Although there have to be some constraints regarding that mode, it is crucial to critically examine wether those constraints are 
absolutely necessary, and if so, if they are implemented in a way that is anthropocentric.

The purpose of this experiment is to examine the possible ways that intelligences can comminicate, and any influence that stems from
a human-centric way of thinking will taint the results.

What is needed: 

 # A definition of knowledge, understanding, truth and communication *in respect to this experiment*, obviously, since we're not trying to solve the entirety of philosophy and metaphysics. (Even though we wouldn't mind.)

 # An agent capable of having knowledge, forming understanding, deciding truth and communicating with other agents.

 # A way to evaluate the understanding of the agents.

 # Constraints that prohibit obvious shortcuts the agents can take

 # A dataset of knowledge (or a way to generate such) that is not just random noise or domain-specific. Otherwise the agents either are helpless fighting kolmogorov complexity or are just cheap copies of existing, better solutions for domain-specific tasks.
 
 	 # This might be especially tricky, since there is a fine line between basic information suitable for this task andinformation that requires an Agent basically capable of human-like reasoning and cognition.

   # On the other hand, if we can define knowledge parametrized in a way that allows for adjustable complexity, this alone could be a very useful tool in agent-based AI research.
